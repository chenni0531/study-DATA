# 9_적합대안 - 수축

#### **Shrinkage 방법**

- 계수 추정치를 제한(constrains)하거나 규칙화(regularizes)하는 기법을 사용하여 p개의 설명변수 모두를 포함하는 모델 적합할 수 있다.
- 계수 추정치들을 수축하는 것은 추정치들의 분산을 상당히 줄일 수 있는 것으로 밝혀져 있다.
- 회귀계수들을 영으로 수축하기 위한 두 가지 가장 잘 알려진 기법은 능형회귀(ridge regression)과 라쏘회귀(lasso regression)이다.

**1. 능형회귀**



![img](https://blog.kakaocdn.net/dn/bjwn12/btq3htNLadE/fLrzoYgWkgwInfCN4zNas0/img.png)능형회귀 계수 추정치는 다음 식을 최소로 하는 값이다.



- 약간 다른 수량을 최소화하여 계수들을 추정한다는 점을 제외하면 최소제곱과 아주 유사하다.
- lambda=0일 때 페널티 항의 영향이 없어 능형회귀는 최소제곱 추정치를 제공한다. 하지만 lamda가 무한대에 가까워짐에 따라 수축 페널티의 영향이 커져 능형회귀 계수 추정치는 0으로 근접한다.
- 단 하나의 계수 추정치들의 집합을 생성하는 최소제곱과 달리, 능형회귀는 lambda의 값 각각에 대해 다른 집합의 계수 추정치를 생성한다.

 

**능형회귀가 최소제곱보다 나은 이유**

- 최소제곱에 대한 능형회귀의 장점은 **편향-분산 절충**에 있다. lambda가 증가하면 능형회귀 적합의 유연성이 감소하게 되어 분산은 감소하지만 편향은 증가한다.
- 변수의 수 p가 관측치의 수 n만큼 클 때 최소제곱 추정치들은 변동이 아주 클 것이다. 또한 p>n이면 최소제곱 추정치는 심지어 유일한 해도 가지지 않지만 능형회귀는 약간의 편향 증가로 분산을 크게 감소하도록 절충하여 잘 동작할 수 있다. 따라서 능형회귀는 최소제곱 추정치가 높은 분산을 가지는 상항에서 가장 잘 동작한다.
- 능형회귀는 2^p개의 모델을 검색해야 하는 **부분집합 선택에 비해 상당한 계산상의 장점**이 있다.

**하지만 한 가지 분명한 단점이 있다**

- 일반적으로 변수들의 서브셋만 포함하는 모델들을 선택하는 최상의 부분집합 선택과 단계적 선택과 달리, 능형회귀는 최종 모델에 p개 설명변수 모두를 포함할 것이다. **모든 계수들은 0을 향해 수축할 뿐 0이 되지 않을 것**이기 때문이다.
- 이것을 예측 정확도에 있어서는 문제가 되지 않을 수 있지만 변수의 수 p가 상당히 큰 설정에서 모델을 해석하는 데 어려움을 초래할 수 있다.

**2. 라쏘회귀**



![img](https://blog.kakaocdn.net/dn/th9YW/btq3mdXeMLY/IFCQaLpJYdAYezWAzYzWtk/img.png)



- 능형회귀에서와 같이 계수 추정치들을 0으로 수축하지만, lasso에서는 L1 페널티는 lambda가 충분히 클 경우 계수 추정치들의 일부를 **정확히 0이 되게 하는 효과**를 갖는다.



![img](https://blog.kakaocdn.net/dn/lUsFP/btq3mYkXFp6/R5xNysHqcOI5TyM4viFLpK/img.png)왼쪽이 라쏘회귀, 오른쪽이 능형회귀에 대한 오차의 등고선과 제한함수. 파란색 영역은 제한영역이고 붉은색 타원들은 RSS의 등고선이다.

<br>

---

<br>

#### **릿지(Ridge)회귀**

```
x=model.matrix(Salary~., Hitters)[,-1]
y=Hitters$Salary

library(glmnet)
grid=10^seq(10, -2, length=100)
ridge.mod=glmnet(x, y, alpha=0, lambda=grid)
```

- **glmnet라이브러리의 glmnet모델**을 사용해 릿지회귀와 라쏘회귀를 적합할 수 있다.
- alpha인자가 0이면 릿지회귀를 적합하고, 1이라면 라쏘모델을 적합한다.
- glmnet함수는 **기본적으로 변수들을 표준화하여 scale이 동일**하게 한다. 기본 설정을 끄려면 standardize=FALSE 옵션을 사용하면 된다.

```
dim(coef(ridge.mod))
##[1]  20 100
```

- coef의 행렬의 경우, 20행(각 설명변수와 절편에 대해 하나씩)과 100열(각 lambda값에 대해 하나씩)을 가진다.

```
ridge.mod$lambda[50]
##[1] 11497.57
coef(ridge.mod)[,50]
##  (Intercept)         AtBat          Hits         HmRun          Runs 
##407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 
##          RBI         Walks         Years        CAtBat         CHits 
##  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 
##       CHmRun         CRuns          CRBI        CWalks       LeagueN 
##  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 
##    DivisionW       PutOuts       Assists        Errors    NewLeagueN 
## -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 
sqrt(sum(coef(ridge.mod)[-1,50]^2))
##[1] 6.360612
```

- lambda가 11,498일 때의 계수들과 이들의 L2 norm을 보여준다.

```
predict(ridge.mod, s=50, type='coefficients')[1:20,]
##  (Intercept)         AtBat          Hits         HmRun          Runs 
## 4.876610e+01 -3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00 
##          RBI         Walks         Years        CAtBat         CHits 
## 8.038292e-01  2.716186e+00 -6.218319e+00  5.447837e-03  1.064895e-01 
##       CHmRun         CRuns          CRBI        CWalks       LeagueN 
## 6.244860e-01  2.214985e-01  2.186914e-01 -1.500245e-01  4.592589e+01 
##    DivisionW       PutOuts       Assists        Errors    NewLeagueN 
##-1.182011e+02  2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00 
```

- **predict함수**와 **s=50**옵션을 설정하여 다음과 같이 새로운 lambda값 50에 대한 릿지회귀계수를 얻을 수 있다.

```
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

ridge.mod=glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2)
##[1] 142199.2
mean((mean(y[train])-y.test)^2) #절편만 가진 모델의 MSE
##[1] 224669.9
```

- **thresh옵션**은 수렴의 기준값이 된다. default는 1e-7이지만, 여기에선 1e-12로 더 작은 값을 설정했다.
- lambda가 4인 릿지회귀를 적합하고 검정셋으로 MSE를 계산한 결과다. lambda가 4인 릿지회귀모델을 적합하는 것이 절편만 가진 모델을 적합하는 것보다 훨씬 낮은 MSE를 초래한다.

```
set.seed(1)
cv.out=cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
##[1] 326.0828

ridge.pred=predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y.test)^2)
##[1] 139856.6

out=glmnet(x, y, alpha=0)
predict(out, type='coefficients', s=bestlam)[1:20,]
## (Intercept)        AtBat         Hits        HmRun         Runs 
## 15.44383135   0.07715547   0.85911581   0.60103107   1.06369007 
##         RBI        Walks        Years       CAtBat        CHits 
##  0.87936105   1.62444616   1.35254780   0.01134999   0.05746654 
##      CHmRun        CRuns         CRBI       CWalks      LeagueN 
##  0.40680157   0.11456224   0.12116504   0.05299202  22.09143189 
##   DivisionW      PutOuts      Assists       Errors   NewLeagueN 
##-79.04032637   0.16619903   0.02941950  -1.36092945   9.12487767 
```



![img](https://blog.kakaocdn.net/dn/b5QzDt/btq3vUWpsXa/Mz8itold79iaA3TEoO8Ot1/img.png)



- **cv.glmnet함수**를 사용하여 조율 파라미터를 선택할 수 있다. **기본적으로 10-fold교차검증**을 수행하는데, **nfolds**인자를 사용하여 변경이 가능하다.
- **cv.glmnet의 lambda.min**을 통해 교차검증 오차가 가장 작은 lambda값을 구할 수 있다.
- lambda가 4일 때 얻은 검정 MSE보다 더 나은 결과인 것을 확인할 수 있다.
- 마지막으로 교차검증에 의해 선택된 lambda값을 사용하여 릿지회귀를 적합하고, **predict에 type='coefficients'옵션을 추가하여 계수 추정치를 출력**했다.

#### **라쏘(LASSO)회귀**

```
lasso.mod=glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
```



![img](https://blog.kakaocdn.net/dn/pqNtz/btq3qNKu6nb/VOjC4QkkBQla3gLSGvhVQK/img.png)



- 계수 그래프를 통해 조율 파라미터의 선택에 따라 몇몇 계수는 그 값이 정확하게 0이 됨을 알 수 있다.

```
set.seed(1)
cv.out=cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)

bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2)
##[1] 143673.6
```



![img](https://blog.kakaocdn.net/dn/zvjap/btq3qMkwBtD/fkWlsuMbdCIXikN3GBDtZ1/img.png)



- 라쏘회귀 적합 결과, 교차검증으로 선택된 lambda를 사용한 능형회귀의 검정 MSE와 거의 같다.

```
out=glmnet(x, y, alpha=1, lambda=grid)
lasso.coef=predict(out, type='coefficients', s=bestlam)[1:20, ]
lasso.coef
##  (Intercept)         AtBat          Hits         HmRun          Runs 
##   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 
##          RBI         Walks         Years        CAtBat         CHits 
##   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 
##       CHmRun         CRuns          CRBI        CWalks       LeagueN 
##   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 
##    DivisionW       PutOuts       Assists        Errors    NewLeagueN 
##-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 

lasso.coef[lasso.coef!=0]
##  (Intercept)         AtBat          Hits         Walks         Years 
##   1.27479059   -0.05497143    2.18034583    2.29192406   -0.33806109 
##       CHmRun         CRuns          CRBI       LeagueN     DivisionW 
##   0.02825013    0.21628385    0.41712537   20.28615023 -116.16755870 
##      PutOuts        Errors 
##   0.23752385   -0.85629148 
```

- 이 예에서 19개의 계수 추정치 중 12개가 0이다. 따라서, 교차검증으로 선택된 lambda값의 lasso모델은 단지 7개의 변수를 포함한다.
