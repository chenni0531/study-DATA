# 의사결정나무

> Decision Tree

[TOC]

**한 번에 하나씩의 설명변수를 사용하여 예측가능한 규칙들의 집합을 생성하는 모델**

- 특정 기준에 따라 데이터를 구분하는 모델

- 예측값의 종류는 터미널 노드의 개수와 일치

```
- 범주형 (분류 트리)
새로운 데이터가 특정 터미널 노드에 속한다는 정보 확인 → 해당 터미널 노드에서 가장 빈도가 높은 범주에 새로운 데이터를 분류

- 연속형 (회귀 트리)
해당 터미널 노드의 종속변수의 평균을 예측값으로 반환
```

| 장점                                                         | 단점                                                 |
| ------------------------------------------------------------ | ---------------------------------------------------- |
| - if-else의 조건문 형식으로 이해하기 쉽고 속도 빠르다<br>- 화이트박스모델: 시각화와 해석이 용이<br>- 여러 피처 간의 상호작용과 연관성 잘 표현<br>- 다양한 데이터 유형: 범주형과 연속형 모두 가능 | 예측력이 떨어진다: 평균 또는 다수결에 의해 예측 수행 |

<br>

---

<br>

## 1. 분리 기준

```
각 노드마다 질문을 던지고 응답에 따라 가지를 쳐서 데이터 분리

분리 후 각 영역의 순도 Homogeneity 증가, 불순도 Impurity / 불확실성 uncertainty 감소하는 방향으로 학습 진행
```

<br>

### 1-1. 불순도 Impurity

**해당 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지**

- 데이터가 잘 분리되었는지의 평가 기준

- 자식 노드들의 불순도가 낮으면 잘 분리된 것

순도: 데이터가 균일한 정도를 나타내는 지표

노드에 여러 분류가 섞여 있으면 높고 하나의 분류만 있다면 낮음

`불순도를 최소화하는 방향으로 학습`

<br>

### 1-2. 엔트로피 Entropy

불순도 지표: 1이면 불순도 최대치, 0이면 불순도 최소

`엔트로피지수를 최소화시키는 방향으로 학습`

![엔트로피](README.assets/엔트로피.jpg)

<br>

### 1-3. 지니 지수 Gini Index

불순도 지표: 노드에 특정 분류i만 있거나 특정 분류i가 전혀 없을 때 I(A)가 작은 값, 반반씩 섞여있을 때 가장 큰 값

`지니지수를 최소화시키는 방향으로 학습`

![지니지수](README.assets/지니지수.jpg)

<br>

### 1-3. 정보 획득 Information Gain

분기 이전의 엔트로피에서 분기 이후의 엔트로피를 뺀 수치

- 어느 feature의 어느 분기점에서 정보 획득이 최대화되는지 판단하여 분기

- 순도 증가, 불확실성 감소 = 정보 획득

`정보 획득을 최대화하는 방향으로 학습`

<br>

### 1-4. 카이제곱 통계량

양 쪽의 클래스 분포 차이를 크게 하는 분리 기준

`카이제곱 통계량을 최대화하는 방향으로 학습`

![카이제곱](README.assets/카이제곱.jpg)

<br>

▶ 한 클래스를 잘 분리해내고 싶다면 지니지수와 엔트로피 지수

▶ 설명 변수가 타겟 클래스 분포에 미치는 영향 확인하고 싶다면 카이제곱 통계량 사용

<br>

---

<br>

## 2. 모형 만들기

### 2-1. 성장

>  Gowing (재귀적 분기)

1. 분할하기 가장 좋은 feature를 찾은 뒤 해당 feature를 기준으로 데이터를 분할
2. 더 이상 정보획득이 없을 때 / 정지 규칙 만족할 때까지 이 과정을 분할 후의 각 그룹에 대해 반복

3. 예측: 해당 데이터를 트리에 전달한 뒤 리프로드에 도달했을 대 해당 노드의 가장 많은 비율을 차지하는 label로 예측

<br>

### 2-2. 가지치기

> Pruning

트리에 가지가 너무 많다면 오버피팅: 예측 성능인 일반화 능력이 떨어질 수 있다

최대 깊이나 터미널 노드의 최대 개수, 한 노드가 분할하기 위한 최소 데이터의 수를 제한

- 사전 가지치기

- 사후 가지치기
  1. Data segmentation: 데이터를 training/tes가 아니라 training/pruning/test 데이터
  2. Complete decision tree with training data: Training data만으로 의사결정 나무
  3. Pruning with test data: Pruning data로 가지치기 수행

- **Cost Complexity Pruning (CCP)**: 비용 복잡도 Cost Complexity를 최소화시키는 과정
- **Reduced Error Pruning (REP)**: 훈련 데이터를 통해 의사결정나무를 만든 뒤 검증 데이터를 통해 모든 중간 마디에 대해 분리 전후 불순도를 비교해가며 가지치기하는 과정 

<br>

---

<br>

## 3. ID3

> Iterative Dichotomiser 3

가장 기본적인 의사결정나무의 알고리즘

- 반복적으로 이분하는 알고리즘

- 불순도 지표로 엔트로피 사용: 정보획득이 가장 큰 지표 선택하여 분기

<br>

## 4. C4.5

- 정교한 불순도 지표 활용: Information Gain Ratio (Information Gain / Intrinsic Value)
- 범주형 변수 뿐 아니라 연속형 변수 사용 가능
- 결측치가 포함된 데이터도 사용 가능
- 과적합 방지를 위한 사후 가지치기

<br>

## 5. 분류와 회귀 나무(CART)

> Classification and Regression Trees

- 불순도 지표로 지니계수 사용: 모든 조합에 대해 지니 계수를 계산 후 지니 계수가 가장 낮은 지표를 찾아 분기한다
- Binary Tree: 가지 분기 시 두 개의 노드로 분기한다
- Regression Tree: 회귀트리를 지원한다 -> 실제값과 예측값의 오차를 사용한다

![Regression Tree 예시 3](https://tyami.github.io/assets/images/post/ML/2020-10-05-CART/2020-10-05-regression-tree-example3.png)

<br>

## 6. 조건부 추론 나무 

> Conditional Inference Tree

통계적 유의성에 대한 판단 없이 노드를 분할하는데 따른 과적합 문제, 다양한 값으로 분할 가능한 변수가 다른 변수에 비해 선호되는 문제

- 조건부 분포에 따라 변수와 반응값 (분류) 사이의 연관관계를 측정하여 노드 분할에 사용할 변수를 선택
- 노드 반복 분할에서 발생하는 다중 가설 검정을 고려한 절차를 적용하여 적절한 시점에 노드 분할 중단

