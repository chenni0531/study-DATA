# 앙상블

[TOC]

**주어진 데이터로부터 여러 개의 모델을 학습한 다음 여러 모델의 예측 결과들을 종합하여 정확도를 높이는 기법**

- 서로 다른 모델을 조합해서 최고의 성능을 내는 모델 생성
- 각각의 분류기는 상호독립적이며 오분류율이 50%보다 낮아야한다

![img](https://blog.kakaocdn.net/dn/cdqN8c/btqw7vHPtvK/mz3GMGa08N3q5JUKz0TeK0/img.png)

- 장점: 단일 모형보다 신뢰성 높은 예측값, 이상치에 대한 대응력 상승, 전체적인 분산 감소
- 단점: 모형의 투명성이 떨어져 원인 분석 어려움

| 앙상블의 방법             | 내용                                                         |               |
| ------------------------- | ------------------------------------------------------------ | ------------- |
| 데이터를 조정하는 방법    | 적절한 표본 추출 방법으로 여러 개의 훈련 데이터 집합 생성하고 여러 개의 분류기를 생성하여 앙상블 진행 | 배깅, 부스팅  |
| 변수의 개수 조절하는 방법 | 전체 변수 집합에서 부분 변수 집합 선택하여 훈련 데이터 생성하고 각각의 훈련 데이터 집합에 대해 분류기를 생성한 후 앙상블 진행 | 랜덤 포레스트 |
| 집단명을 조절하는 방법    | 집단의 종류가 많을 경우 소수의 집단만으로 묶어서 분류        |               |

<br>

<br>

## 1. 배깅 

> Bagging; Bootstrap aggregating

**데이터로부터 여러 개의 부트스트랩 데이터를 추출해 모델링한 후 결합하여 최종의 예측 모델을 생성하는 방법**

`부트스트랩 데이터`: 단순 복원 임의 추출법을 통해 원 데이터로부터 크기가 동일한 여러 개의 표본 데이터

- 훈련 데이터 셋에서 임의로 하위 데이터 셋을 추출하여 생성

- 중복을 허용하기 때문에 단일 데이터가 여러 번 선택될 수 있다

```
회귀트리: B개의 부트스트랩된 훈련 데이터 셋을 사용하여 B개의 회귀트리를 만들고 그 예측 결과들을 평균
분류트리: B개의 트리 각각에 의해 예측된 클래스를 기록하고 다수결로 결정
```

- 통계학습방법의 분산을 줄이기 위한 범용 절차: 복원 추출을 통해 예측 모형의 분산 최소화 -> 예측력 향상
- 과적합되었거나 편의가 작고 분산이 큰 모형에 적합

| 장점                                    | 단점                                                         |
| --------------------------------------- | ------------------------------------------------------------ |
| 각 트리들의 편향을 유지하면서 분산 감소 | 같은 데이터에서 복원 추출을 하므로 공분산 존재<br>트리가 증가할수록 전체 분산 증가 |

랜덤포레스트와 다른 점은 `전체 데이터`를 여러 번 복원 추출 (부트스트래핑) 한다는 점

<br>

## 3. 랜덤 포레스트

> RandomForest

**의사결정나무를 만들 때 데이터의 일부를 복원 추출한 데이터에 대해서만 의사결정나무를 만드는 방식**

배깅의 단점 보완: 배깅 프로세스로 다수의 나무를 만드는데 공분산을 줄이기 위해 데이터 뿐 아니라 변수까지 랜덤으로 뽑는 것

- 변수의 개수는 전체 변수 개수의 제곱근
- 트리들의 상관성을 제거하는 방법
- 각 의사결정나무는 데이터의 일부만을 사용: 분리 기준을 정할 때 전체 변수가 아니라 일부 변수만 대상

```
1. 각 분할에서 전체 속성들 중 일부만 고려하여 트리를 작성한다
   p개의 설명변수 집합에서 m개의 설명변수들로 구성된 랜덤 표본 사용: 나무에 다양성 
2. 훈련을 통해 구성해 놓은 다수의 나무들로부터 분류 결과를 취합하여 결론을 얻는다
3. 새로운 데이터에 대한 예측을 수행할 때는 여러 개의 의사결정나무의 예측 결과를 투표 방식으로 합한다
```

성능 뛰어나고 의사결정나무 여러 개를 사용하여 과적합 문제 피한다: 다수의 나무들로부터 분류를 집계하므로 오버피팅이 나타나는 나무의 영향력 줄일 수 있다

<br>

## 2. 부스팅

> Boosting

**약한 예측 모델을 결합하여 강한 예측 모델 생성하는 방법**

```
1. 원데이터에 동일 가중치로 모델 생성
2. 생성된 모델의 오분류 데이터 수집 
3. 오분류 데이터에 높은 가중치 부여: 앞에서 예측한 분류기가 틀린 부분에 가중치를 부여 → 틀린 부분을 더 잘 맞출 수 있도록
4. 과정 반복으로 모델 정확도 향상
```

- 분류 모델을 개선하기 위한 목적

- 트리들이 순차적으로 만들어진다: 이전의 트리 정보를 사용하여 만들어지므로 이미 만들어진 트리에 강하게 의존

배깅처럼 복원 임의 추출 사용하지만 `가중치`를 부여한다: 배깅이 일반적인 모델 만드는데 집중한다면 부스팅은 맞추기 어려운 문제를 맞추는데 집중

![img](https://blog.kakaocdn.net/dn/dQDi7Z/btqw7O8d309/kUC7v0dO1FTB97VuQPWMgk/img.png)

