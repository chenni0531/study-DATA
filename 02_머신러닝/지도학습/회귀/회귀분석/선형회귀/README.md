# 선형회귀

[TOC]

종속 변수(응답 변수) y와 한 개 이상의 독립 변수 (설명 변수) X와의 선형관계를 모델링하는 회귀분석

<br>

## 1. 선형회귀 조건

> 모형의 선형성, 오차의 등분산성/정규성/독립성, 설명변수 간의 독립성

- **모형의 선형성**: 독립변수와 종속변수는 선형관계이다
- **오차의 등분산성/정규성**: 오차항은 평균이 0이고 분산이 일정한 정규분포를 갖는다

- **오차의 독립성**: 오차항은 자기 상관성이 없다

- **설명변수 간의 독립성**: 설명변수들은 서로 선형적으로 독립니다
- 설명변수와 오차항은 서로 독립이다

- 데이터에 아웃라이어가 없다

```
- 결측값, 중복값 처리
- 설명변수 간 다중공선성: 산점도, 상관계수, 분산팽창지수
- 설명변수 표준화, 척도 변환
- 질적 설명 변수: 더미변수 사용
```

<br>

---

<br>

## 2. 피어슨 상관 계수

> Pearson correlation coefficient

독립 변수와 종속 변수 간의 상관관계를 나타내는 척도를 상관 계수

-  -1부터 1 사이의 값으로 표시
- **선형 관계의 강도(Strength)**

| Strong            | Moderate          | Weak              |
| ----------------- | ----------------- | ----------------- |
| 0.7 ≤ \|r\| ≤ 1.0 | 0.3 ≤ \|r\| < 0.7 | 0.0 ≤ \|r\| < 0.3 |

- **선형 관계의 방향(Direction)**:1이면 양의 상관관계, -1이면 음의 상관관계

| Positive | Negative |
| -------- | -------- |
| r > 0    | r < 0    |

<br>

---

<br>

## 3. 적합 방법

### 3-1. 최소제곱법

> 최소자승법: Least Square Estimation Method, OLS (Ordinary Least Squares)

어떤 데이터가 주어졌을 때 최적의 추세선을 그리기 위한 방법 

잔차의 제곱합(RSS: Residual Sum of Square)을 최소로 하는 방법

잔차의 제곱합을 편미분하여 연립 방정식을 푸는 방법

**잔차(Residual)  = Loss**: 실제 데이터의 y값(실제값)과 회귀직선의 y값(예측값)의 차이

<br>

### 3-2. 경사하강법

> Gradient Descent

loss가 가장 크게 감소하는 방향으로 `m` 과 `b` 를 이동시키는

loss를 최소화하기 위한 방법

**일단 파라미터를 임의로 정한 다음에 조금씩 변화시켜가며 손실을 점점 줄여가는 방법으로 최적의 파라미터를 찾아간다.** (그리고 이 때 미분이 사용되는데 당연히 직접 할 필요 없으니 겁먹지 말자.)



경사하강법 같은 반복적인 방식으로 선형회귀 계수를 구할 수도 있습니다. 경사하강법이란 어떤 함수값을 최소화하기 위해 임의의 시작점을 잡은 후 해당 지점에서의 그래디언트(경사)를 구하고, 그래디언트의 반대 방향으로 조금씩 이동하는 과정을 여러번 반복하는 것입니다. 

 경사하강법 가운데 **Stochastic Gradient Descent(SGD)** 기법을 쓰겠습니다. SGD는 반복문을 돌 때마다 **개별 데이터 포인트에 대한 그래디언트를 계산**하고 이 그래디언트의 반대 방향으로 파라메터를 업데이트해 함수의 최소값을 구하는 기법입니다.



학습률 (Learning Rate)

이렇게 학습률을 크게 설정하면 최적의 값을 제대로 찾지 못한다. 일을 대충하는 거다. 대신 일을 빨리 하긴 하겠지.

그렇다고 학습률을 작게 설정하면 최적의 값으로 수렴할 때까지 시간이 오래 걸린다.

그래서 모델을 학습시킬 때는 최적의 학습률을 찾는 게 중요하다. 효율적으로 파라미터를 조정하면서도 결국 최적의 값을 찾아 수렴할 수 있을 수준으로.



coefficient

회귀계수의 의미



회귀계수에 대한 검정

귀무가설: 회귀계수는 0이다 -> 변수의 설명력이 없다

대립가설: 회귀계수는 0이 아니다 -> 변수의 설명력이 존재한다



## 회귀계수를 얼마나 신뢰할 수 있나

데이터로부터 추정한 회귀계수는 진짜 회귀계수라고 할 수는 없습니다. 어디까지나 일부 데이터를 가지고 도출된 계수일 뿐더러 데이터에 노이즈가 끼어있을 수도 있기 때문이죠. 선형회귀 모델의 jj번째 독립변수에 대한 추정 회귀계수 βjβj를 추정 회귀계수의 표준편차 σjσj로 나눈 값(tjtj)은 n−kn−k의 자유도를 지닌 tt분포를 따른다는 사실이 알려져 있습니다.

그렇다면 회귀계수의 표준편차는 어떻게 구할까요? 이 때 쓰는 것이 **bootstap**입니다. 기존 데이터에서 중복을 허용된 재추출을 통해 새로운 데이터를 만들어내는 방법입니다. 우리가 갖고 있는 학습데이터에 bootstrap을 적용하여 여러 새로운 데이터를 생성하고, 이들 각각으로부터 추정 회귀계수를 도출할 수 있습니다.

예를 들어 특정 독립변수에 해당하는 회귀계수가 부트스트랩 데이터마다 크게 달라지지 않는다면 해당 회귀계수는 상당히 신뢰할 수 있을 겁니다. 반대로 계수가 크게 변한다면 추정된 계수는 신뢰할 수 없을 겁니다.

부트스트랩 데이터 개수만큼 회귀계수 추정값을 구하여 회귀계수의 표준편차 σjσj를 구한 뒤 이를 원래 데이터에서 구한 추정회귀 계수 βjβj에 나눠줘 tjtj를 계산합니다. 이 tjtj가 tt분포 상에서 어느 위치에 해당하는지를 따져서 진짜 회귀계수가 속해 있을 수 있는 신뢰구간을 구할 수 있게 됩니다.

부트스트랩 데이터 각각에 대해 회귀계수를 먼저 구해보겠습니다. 우선 데이터로부터 회귀계수를 추정하는 함수를 만들었습니다.

## 정규화

**Regulization**은 회귀계수에 제약을 가해 일반화 성능을 높이는 기법입니다. 정규화와 관련해서는 [이곳](https://ratsgo.github.io/machine learning/2017/05/22/RLR/)을 참고하시면 좋을 것 같습니다. 이 가운데 ββ의 L2 norm을 제한하는 릿지 회귀를 살펴보겠습니다. 파이썬 코드는 다음과 같습니다.
