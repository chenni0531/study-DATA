# 선형회귀

[TOC]

종속 변수(응답 변수) y와 한 개 이상의 독립 변수 (설명 변수) X와의 선형관계를 모델링하는 회귀분석

- 단순회귀분석(Simple Regression Analysis)

- 다중회귀분석(Multi Regression Analysis)

<br>

## 1. 선형회귀 조건

> 모형의 선형성, 오차의 등분산성/정규성/독립성, 설명변수 간의 독립성

- **모형의 선형성**: 독립변수와 종속변수는 선형관계이다 `▶ 회귀선 확인`
- **오차의 등분산성/정규성**: 오차항은 평균이 0이고 분산이 일정한 정규분포를 갖는다 `▶ 표준잔차 확인, 정규성 검정 확인`

- **오차의 독립성**: 오차항은 자기 상관성이 없다 `▶  더빈-왓슨 값 확인`

- **설명변수 간의 독립성**: 설명변수들은 서로 선형적으로 독립이다 `▶ 분산팽창요인 VIF 확인`
- 설명변수와 오차항은 서로 독립이다

- 데이터에 아웃라이어가 없다

```
- 결측값, 중복값 처리
- 설명변수 간 다중공선성: 산점도, 상관계수, 분산팽창지수
- 설명변수 표준화, 척도 변환
- 질적 설명 변수: 더미변수 사용
```

<br>

---

<br>

## 2. 피어슨 상관 계수

> Pearson correlation coefficient

독립 변수와 종속 변수 간의 상관관계를 나타내는 척도를 상관 계수

-  -1부터 1 사이의 값으로 표시
- **선형 관계의 강도(Strength)**

| Strong            | Moderate          | Weak              |
| ----------------- | ----------------- | ----------------- |
| 0.7 ≤ \|r\| ≤ 1.0 | 0.3 ≤ \|r\| < 0.7 | 0.0 ≤ \|r\| < 0.3 |

- **선형 관계의 방향(Direction)**:1이면 양의 상관관계, -1이면 음의 상관관계

| Positive | Negative |
| -------- | -------- |
| r > 0    | r < 0    |

<br>

---

<br>

## 3. 회귀 모형 구축

- 설명모형(Explanatory Model): 종속변수와 독립변수들 간의 관계를 설명
  - 데이터가 적은 모형
  - 변수 간의 관계를 잘 설명할 수 있는 최상의 적합모형을 추정

- 예측모형(Predictive Model): 새로운 데이터에 대한 결과값 예측
  - 데이터가 충분히 많은 모형
  - 적합된 모형을 이용하여 새로운 데이터 예측
  - Training data(모형추정)와 Test data(모형성능 평가)로 나뉘어 분석

<br>

### 3-1. 최소제곱법 OLS

> 최소자승법: Least Square Estimation Method, OLS (Ordinary Least Squares)

어떤 데이터가 주어졌을 때 최적의 추세선을 그리기 위한 방법 

```
회귀식의 기울기와 절편을 구하는 방법
```

- 잔차의 제곱합(RSS: Residual Sum of Square)을 최소로 하는 방법

- 잔차의 제곱합을 편미분하여 연립 방정식을 푸는 방법

- **잔차(Residual)  = Loss**: 실제 데이터의 y값(실제값)과 회귀직선의 y값(예측값)의 차이

<br>

### 3-2. 경사하강법

> Gradient Descent

어떤 함수값을 최소화하기 위해 임의의 파라미터 (시작점)을 정하고 조금씩 변화시켜가며 Loss를 점점 줄여가는 방법으로 최적의 파라미터를 찾는 방법

```
loss를 최소화하는 최적의 기울기와 절편을 구하는 방법
```

- 그래디언트 gradient: 특정 시점에서의 기울기 (미분)
- 해당 지점에서의 그래디언트(경사)를 구하고 그래디언트의 반대 방향으로 조금씩 이동하는 과정 반복

- Loss가 가장 크게 감소하는 방향으로 기울기`w` 과 절편 `b` 를 이동시키는 것

- **Convergence**: Loss의 변화가 거의 없는 iterations 지점

- **Learning rate**: 한 step에서 gradient가 변화하는 양
  - 너무 작은 learning rate: converge까지 많은 시간 소요
  - 너무 큰 learning rate: 가장 적은 loss값을 지나쳐 발산하는 경우 발생

<br>

---

<br>

## 4. 회귀계수

> coefficient

회귀계수에 대한 검정

- 귀무가설: 회귀계수는 0이다 ▶ 변수의 설명력이 없다

- 대립가설: 회귀계수는 0이 아니다 ▶ 변수의 설명력이 있다

<br>

---

<br>

## 5. 정규화

> Regularization

회귀계수에 제약조건을 부여하는 방법

- 모형이 과도하게 최적화되는 현상인 과적합을 막는 방법

- Variance를 감소시켜 일반화 성능을 높이는 기법

- Ridge, Lasso, Elastic Net

