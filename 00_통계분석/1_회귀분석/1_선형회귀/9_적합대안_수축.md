# 7_변수 선택

선형 회귀 모델을 만들 때 주어진 여러 변수 중 어떤 변수를 설명 변수로 해야 할지 선택하는 과정

F 통계량이나 AIC 를 사용해 변수를 하나씩 택하거나 제거하는 것

<br>

---

<br>

## 1. 모델 비교

최적의 모형을 찾기: 변수 추가 및 제거의 타당성 검토

- **단일 독립변수의 유의성**: 하나의 독립변수가 반응변수를 유의하게 설명하는지

- **여러 독립변수의 유의성**: 여러 독립변수가 반응변수를 유의하게 설명하는지

- **Big Model vs Small Model**: 변수가 많은 모형과 그 모형에서 변수 몇 개를 제외한 모형의 비교

검정 통계량 식 그림 첨부

<br>

### 1-1. AIC

> Akaike Information Criterion; 아카이키 정보 기준

모형의 정확성 (accuracy)과 복잡성 (complexity)에 관한 정보 엔트로피

식 그림 첨부

AIC가 작은 모형이 우수

변수가 많을수록 모형의 적합성 (fitness) ↑ 복잡성 (complexity) ↑

- `k`: 모형에 사용된 매개변수의 개수 -> 값이 작을수록 모형 간단
- `ln(L)`: 가능도(우도; likelihood) 함수에 log를 취한 것 -> 값이 클수록 추정된 모형의 적합성 높음

<br>

### 1-2. Deviance

> 이탈도, 편차

적합 정도를 나타내는 척도

식 그림 첨부

변수가 많을수록 모형의 적합성 (fitness) ↑ 이탈도 (deviance) ↓

<br>

### 1-3. 수정된 결정계수

> Adjusted coefficient of determination

기존의 R2이 불필요한 변수를 추가시키더라도 증가하는 성향과 설명변수의 수를 반영하지 못하는 단점을 보완

그림 첨부

<br>

### 1-4. 축차 제곱합

> Sequential sum of squares

<br>

---

<br>

## 2. 전진 선택법

> forawrd selection

- 더 이상 유의한 추가 변수가 없을 때까지 변수를 하나씩 추가

- 절편만 있는 모델에서 기준 통계치를 가장 많이 개선시키는 변수를 차례로 추가

```
1. 고려된 변수 중 SSR(Xi)이 가장 높고 유의하면 변수를 선택한다
2. 이미 선택된 설명변수(Xi)의 설명부분 SSR을 제외한 SSR의 증가분 SSR(Xj|Xi)이 가장 크고 설명력이 유의한 경우 Xj 선택
3. 이미 선택된 설명변수(xi, Xj)의 설명부분 SSR을 제외한 SSR의 증가분 SSR(Xl|Xi, Xj)이 가장 크고 설명력이 유의한 경우 Xl 선택
4. 유의한 설명변수가 없을 때까지 반복
```

계산 시간이 빠르지만 선택된 설명변수는 절대 제거되지 않고 중요한 변수가 모형에 진입하지 못할 수도 있다

유의성 검정 과정 첨부

<br>

---

<br>

## 3. 후진 제거법

> Backward Elimination

- 유의하지 않은 독립변수들을 계속 제거

- 모든 변수가 포함된 모델에서 기준 통계치에 가장 도움이 되지 않는 변수를 하나씩 제거

```
1. 고려된 설명변수를 모두 삽입한 후 설명변수 중 가장 유의하지 않은 설명변수를 제거
2. 모든 설명변수가 유의할 때까지 반복
```

<br>

---

<br>

## 4. 단계적 선택

> Stepwise Selection

- 전진 선택법 + 후진 제거법

- 모든 변수가 포함된 모델 / 절편만 포함된 모델에서 출발
- 기준 통계치에 가장 도움이 되지 않는 변수를 삭제 / 빠져 있는 변수 중에서 기준 통계치를 가장 개선시키는 변수 추가

```
1. 고려된 설명 변수 중 설명력 SSR(Xi)이 가장 높고 설명력이 유의하면 변수 선택
2. 이미 선택된 설명변수의 설명부분을 제외한 SSR(Xj|Xi)이 가장 크고 설명력이 유의한 경우 Xj를 선택
3. 새로 선택된 변수의 설명부분을 제외한 부분에 대해 이미 존재한 설명변수의 유의성을 검정하여 유의하지 않으면 제외 / 유의하면 계속 넣고 다음 단계
4. 다시 이미 선택된 변수를 제외하고 다른 변수들 중 가장 SSR의 증가분이 높은 변수를 유의성 검정에 의해 유의하면 변수 추가
5. 유의한 설명변수가 존재하지 않을 때까지 반복
```

중요한 변수가 모형에서 제외될 가능성이 적은 비교적 안전한 방법이지만 한 번 제외된 변수는 다시 선택되지 못한다

<br>

---

<br>

## 5. 최량부분집합

> Best Subsets Regression

변수의 수가 1, 2, ..., k개 일 때, 각각 최고의 모형을 찾고 수정된 R2를 비교해서 최선의 모형을 선택

- 절약모형 (parsimonious model): 변수의 수를 고려한 비교에 의해 간단한 모형을 찾는다

- 변수의 수가 많으면 계산량이 매우 많아진다



