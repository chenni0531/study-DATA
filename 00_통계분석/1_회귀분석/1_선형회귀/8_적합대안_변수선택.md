# 7_적합대안 - 변수선택

[TOC]

선형 회귀 모델 구축에 사용되는 최소제곱법이 아닌 다른 적합절차를 사용하는 이유

- 예측 정확도: n이 p보다 아주 크지 않으면 최소제곱적합에 많은 변동이 존재할 수 있어 과적합을 초래할 수 있다. p>n가 되면 분산이 무한대가 되어 최소제곱 방법은 전혀 사용할 수 없게 된다.
- 모델 해석력: 최소제곱방법으로 반응변수와 관련이 없는 변수들도 정확하게 0인 계수 추정치를 얻게 될 가능성은 거의 없다. 관련없는 변수들을 제외하는 기법들이 필요하다.

→ 서브셋(부분집합) 선택, 수축(shrinkage), 차원축소(Dimension Reduction)

<br>

**변수선택**: 선형 회귀 모델을 만들 때 주어진 여러 변수 중 어떤 변수를 설명 변수로 해야 할지 선택하는 과정으로 F 통계량이나 AIC 를 사용해 변수를 하나씩 택하거나 제거하는 것



## 2. 최상의 부분집합 선택법

```
1. 
```



<br>

---

<br>

## 2. 전진 선택법

> forawrd selection

- 더 이상 유의한 추가 변수가 없을 때까지 변수를 하나씩 추가

- 절편만 있는 모델에서 기준 통계치를 가장 많이 개선시키는 변수를 차례로 추가

```
1. 고려된 변수 중 SSR(Xi)이 가장 높고 유의하면 변수를 선택한다
2. 이미 선택된 설명변수(Xi)의 설명부분 SSR을 제외한 SSR의 증가분 SSR(Xj|Xi)이 가장 크고 설명력이 유의한 경우 Xj 선택
3. 이미 선택된 설명변수(xi, Xj)의 설명부분 SSR을 제외한 SSR의 증가분 SSR(Xl|Xi, Xj)이 가장 크고 설명력이 유의한 경우 Xl 선택
4. 유의한 설명변수가 없을 때까지 반복
```

계산 시간이 빠르지만 선택된 설명변수는 절대 제거되지 않고 중요한 변수가 모형에 진입하지 못할 수도 있다

유의성 검정 과정 첨부

<br>

---

<br>

## 3. 후진 제거법

> Backward Elimination

- 유의하지 않은 독립변수들을 계속 제거

- 모든 변수가 포함된 모델에서 기준 통계치에 가장 도움이 되지 않는 변수를 하나씩 제거

```
1. 고려된 설명변수를 모두 삽입한 후 설명변수 중 가장 유의하지 않은 설명변수를 제거
2. 모든 설명변수가 유의할 때까지 반복
```

<br>

---

<br>

## 4. 단계적 선택

> Stepwise Selection

- 전진 선택법 + 후진 제거법

- 모든 변수가 포함된 모델 / 절편만 포함된 모델에서 출발
- 기준 통계치에 가장 도움이 되지 않는 변수를 삭제 / 빠져 있는 변수 중에서 기준 통계치를 가장 개선시키는 변수 추가

```
1. 고려된 설명 변수 중 설명력 SSR(Xi)이 가장 높고 설명력이 유의하면 변수 선택
2. 이미 선택된 설명변수의 설명부분을 제외한 SSR(Xj|Xi)이 가장 크고 설명력이 유의한 경우 Xj를 선택
3. 새로 선택된 변수의 설명부분을 제외한 부분에 대해 이미 존재한 설명변수의 유의성을 검정하여 유의하지 않으면 제외 / 유의하면 계속 넣고 다음 단계
4. 다시 이미 선택된 변수를 제외하고 다른 변수들 중 가장 SSR의 증가분이 높은 변수를 유의성 검정에 의해 유의하면 변수 추가
5. 유의한 설명변수가 존재하지 않을 때까지 반복
```

중요한 변수가 모형에서 제외될 가능성이 적은 비교적 안전한 방법이지만 한 번 제외된 변수는 다시 선택되지 못한다

<br>

---

<br>

## 5. 최량부분집합

> Best Subsets Regression

변수의 수가 1, 2, ..., k개 일 때, 각각 최고의 모형을 찾고 수정된 R2를 비교해서 최선의 모형을 선택

- 절약모형 (parsimonious model): 변수의 수를 고려한 비교에 의해 간단한 모형을 찾는다

- 변수의 수가 많으면 계산량이 매우 많아진다



