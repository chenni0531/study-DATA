# 데이터 전처리

> 모델링에 알맞은 형태로 데이터를 처리해주기: Data Preprocessing, Data Cleaning

[TOC]

[💻 데이터 전처리 with R](./데이터전처리_R)

<br>

모델에 input 값으로 데이터를 넣기 위해 정제하는 과정

```
데이터 품질
- 정확성: 데이터의 값이 정확한지
- 적시성: 데이터가 최신의 것인지
```

**정형 데이터**:형식이 정해져 있는 데이터

- 데이터베이스에 이미 잘 정리가 되어 있어 바로 통계적 분석에 이용될 수 있는 데이터
- 엑셀 파일에 정리된 인구 통계 데이터 등

**비정형 데이터**: 형식이 정해지지 않은 데이터

- 날 것 그대로의 데이터 raw data

- 사진, 음성, sns나 채팅 속에 등장하는 텍스트 등

- 통계 분석 자료로 쓰일 수 없기 때문에 전처리 과정을 통해 이를 정제 및 가공하여 정형화

<br>

---

<br>

## 1. 데이터 클리닝 (Cleaning)

### 1-1. 결측치 대체

결측치들을 파악하여 제거하거나 새로운 데이터로 채워넣는 과정

- 결측치 제거하기
- 적절한 값으로 대체하기
  - 0, 인접한 값 추정치, 평균으로 대체
  - 범주형 변수 새로 정의

- NA 표시하고 넘기기

<br>

### 1-2. 잡음 데이터 처리

**잡음 (Noise)**: 측정 과정에서 무작위로 발생하여 에러를 발생시키는 것

- 랜덤잡음 : 일정 규칙 없이 랜덤하게 발생하는 잡음 (White Noise)
- 통계적 잡음 : 통계적 분포를 따르는 잡음
- `Moving Average Filter` : window(mask)가 이동하면서 주위 값들에 비해 높거나 낮을 경우 평균 값으로 대체
- `Median Filter` : 일정 범위의 중간 값을 해당 지점의 값으로 지정

<br>

### 1-3. 이상치의 확인 및 제거

**이상치 (Outlier)**: 값의 범위가 일반적인 범위에서 벗어나 특별한 값을 갖는 데이터

**이상치 검출 (Detection)**

![img](https://wikidocs.net/images/page/16582/zscore_od.png)

- `표준점수`: 평균이 0, 표준편차가 1인 분포로 변환한후 +3 이상이거나 -3 이하인 경우
- `IQR`:  75% percentile + 1.5 * IQR 이상이거나 25 percentile - 1.5 * IQR 이하인 경우
- `Variance` : 정규분포에서 97.5% 이상 또는 2.5%의 이하에 포함되는 값
- `Likelihood`  : 베이즈 정리에 의해 데이터 셋이 가지는 두 가지 경우(정상/이상)에 대한 발생 확률(Likelihood)로 이상치 판별
- `Nearest-neighbor` : 모든 데이터 쌍의 거리를 계산하여 이상치 검출
- `Density` : 샘플의 LOF(local outlier factor)를 계산하여 값이 가장 큰 데이터가 이상치
- `Clustering` : 데이터를 여러 클러스터로 구분 후 거리가 먼 클러스터가 이상치

<br>

---

<br>

## 2. 데이터 통합 (Integration)

- 중복 데이터의 제거: 상관 분석 등을 통해 중복된 데이터 및 변수 제거

- 표현 및 스케일의 일치

- 다양한 로그 파일 및 데이터베이스의 통합

<br>

---

<br>

## 3. 데이터 변환 (Transformation)

데이터를 분석하기 좋은 형태로 바꾸는 작업

- **정규화(normalization, Feature Scaling)**: 변수값의 분포를 표준화하는 것
  - z변환: 평균을 0점으로, 표준 편차를 1점으로 환산하는 변환
  - 데이터의 평균을 빼거나 표준 편차로 나누는 작업
- **원 핫 인코딩 One Hot Encoding**: 여러 개의 가변수를 사용해 범주형 변수를 재표현하는 것

- `smoothing`: 노이즈를 제거하기 위해 데이터 추세에 벗어나는 데이터를 변환

- `범주형 변환`: 수치 데이터를 범주형으로 변환하여 사용하는 경우

- `로그 변환`: 로그를 취할 때 정규 분포에 가까워진다면 로그 값을 취한 값을 사용

- `역수 변환`: 역수일 때 선형적인 특성을 가진다면 역수를 사용
- `집합화(Aggregation) 및 요약(summarization)`

<br>

---

<br>

## 4. 데이터 축소 (Reduction)

같은 정보량을 가지면서 데이터의 크기를 줄이는 방법

- **PCA 주성분 분석 (Principle Components Analysis)**
  - 기존 데이터의 특징들을 대표하는 값을 추출
  - 변수들을 선형적인 상관관계가 없는 다른 변수들(주성분)로 재표현
  - 주성분은 원 데이터의 분산을 최대한 보존하는 방법으로 구한다

- `차원 축소 방법`: 변수의 최소 집합을 찾는다 (Stepwise forward selection, Stepwise backward elimination)

- `데이터 이산화(Discretization)`: 수치 데이터를 속성 값으로 변환

  ex) [0~0.5) : Low, [0.5~1.0] : High
- 여러 데이터를 대표할 수 있는 새로운 변수 만들기

<br>

---

<br>

## 5. 샘플링 (Sampling)

전체 데이터 중 분석에 필요한 데이터

- 샘플 데이터가 전체 데이터의 특징을 유지해야한다
- 랜덤한 성질을 보장해야한다
- 훈련 데이터와 테스트 데이터
  - **훈련 (Trainig)**: 모델을 만드는 과정
  - **테스트 (Test)**: 모델의 성능을 검증

<br>

---

<br>

## 6. 변수 선택 (Feature Selection)

주어진 데이터의 변수 중 모델링에 가장 적합한 변수만 택하는 과정

- 필터 방법 Filter Method: 모델링 기법에 의존하지 않고 데이터의 통계적 특성으로부터 변수 선택

  - **0에 가까운 분산 제거 Near Zero Variance**
  - **상관계수 Correlation**: 상관계수가 높은 변수는 독립된 차원으로 변환하거나 제거
    - 상관계수가 큰 예측 변수가 있을 경우 학습 모델의 성능이 떨어지거나 모델이 불안정해진다
    - 차원의 저주: 파라미터 수가 불필요하게 증가한다

  - **카이제곱검정 Chi-Squared Test**: 예측 대상이 되는 변수 간에 독립성을 검정한다
    - 독립이라면 해당 변수는 모델링에 적합하지 않은 것
    - 독립이 아니라면 모델링에 중요한 변수

- 래퍼 방법 Wrapper Method: 변수의 일부만 모델링에 사용하고 그 결과를 확인하는 작업을 반복하면서 변수를 택해나가는 방법

- 임베디드 방법 Embedded Method: 모델 자체에 변수 선택이 포함된 경우 (LASSO)
